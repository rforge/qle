% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/simQLData.R
\name{setQLdata}
\alias{setQLdata}
\title{Set quasi-likelihood (QL) data}
\usage{
setQLdata(runs, X = NULL, var.type = "cholMean", Nb = 0, na.rm = TRUE,
  verbose = FALSE)
}
\arguments{
\item{runs}{list or matrix of simulation results from \code{\link{simQLdata}}}

\item{X}{list or matrix of model parameters}

\item{var.type}{character, "\code{cholMean}" (default), whether to Cholesky decompose variance
matrices either for sample average variance approximation or kriging variance matrices}

\item{Nb}{numeric, number of bootstrap samples for kriging the variance matrix,
only if `\code{var.type}`=`\code{kriging}`, default is zero which uses no bootstrapping}

\item{na.rm}{if \code{TRUE} (default), remove `NA` values from simulation results}

\item{verbose}{if \code{TRUE}, print intermediate output}
}
\value{
An object of class \code{QLdata}  as a data frame with columns:
  \item{X}{ Model parameters (\code{n=1},...,\code{q}) }
  \item{mean}{ Results of simulations (\code{m=1},...,\code{p}) }
  \item{var}{ Simulation variances of statistics (\code{m=1},...,\code{p}) }
  \item{L}{ if applicable, Cholesky decomposed terms of variance matrices of statistics (k=1,...,(m*(m+1)/2))}
	where `\code{p}` denotes the number of user defined statistics and `\code{q}` the problem dimension, that is,
 the number of parameters to be estimated.	

	The following items are stored as attributes:

  \item{type}{ see above}
	 \item{nsim}{ number of simulations at each point }
	 \item{xdim}{ parameter dimension}
	 \item{nWarnings}{ Number of warnings during simulations}
 \item{nErrors}{ Number of errors during simulations}
	 \item{nIgnored}{ List of parameters ignored (because of failures)}
}
\description{
Aggregate the data for quasi-likelihood estimation
}
\details{
The function aggregates all neccessary data for quasi-likelihood estimation storing the
	sample locations and the corresponding simulation results of the statistics.
 If `\code{X}` equals \code{NULL}, then the sample points are taken from the object `\code{runs}`.

 The most critical part is the decomposition of variance matrices for each sample location unless `\code{var.type}`
 equals "\code{const}" in which case a constant variance matrix approximation is expected later by function \code{\link{qle}}.
 The Cholesky decompositions are used for average approximations of the variance matrix of the statistics when calculating the
 quasi-score vector or any type of function criterion based on the Mahalanobis distance or quasi-deviance.
 If these fail for any reason we try to ignore, if possible, the corresponding sample points and exclude them
 from all following calculations. Unless a constant estimate of the variance matrix, the default is to approximate the
 variance at any model parameter by either a kriging interpolation of the \emph{Cholesky} terms or as an average over
 all sampled variance matrices also based on the decomposed Cholesky terms (see vignette).
}
\examples{
# simulate model statistics at LHS design
sim <- simQLdata(sim =
         function(x,cond) {
           X <- rlnorm(cond$n,x[1],x[2])
           c("MED"=median(X),"MAD"=mad(X))
         },
         cond=list("n"=10),
         nsim=10, N=10, method="maximinLHS",
         lb=c("mu"=-1.5,"sd"=0), ub=c("mu"=2,"sd"=1))

# setup the QL data model using defaults
qldata <- setQLdata(sim,verbose=TRUE) 
  
}
\author{
M. Baaske
}
